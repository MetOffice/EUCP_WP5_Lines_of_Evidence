{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import iris\n",
    "from ascend import shape\n",
    "from glob import glob as glob\n",
    "import os\n",
    "# works in SCITOOLS Default/next (2021-03-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(data, ax, edge_color, fill_color, positions, widths):\n",
    "    bp = ax.boxplot(data, patch_artist=True, positions = positions, widths=widths, showfliers=False, whis=[10,90])\n",
    "    \n",
    "    for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "        if element == 'medians':\n",
    "            col = 'black'\n",
    "        else:\n",
    "            col = edge_color\n",
    "        plt.setp(bp[element], color=col)\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color)       \n",
    "        \n",
    "    return bp\n",
    "\n",
    "\n",
    "def bxp(data, ax, colour, alpha, position, width, **kwargs):\n",
    "    bp = ax.bxp(data, patch_artist=True, positions=position, widths=width, showfliers=False, **kwargs)\n",
    "    for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "        if element == 'medians':\n",
    "            col = 'black'\n",
    "        else:\n",
    "            col = colour\n",
    "        plt.setp(bp[element], color=col)\n",
    "        plt.setp(bp[element], alpha=alpha)\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=colour)\n",
    "        patch.set(alpha = alpha)   \n",
    "        \n",
    "    return bp\n",
    "\n",
    "    \n",
    "def create_x_points(ys, basex, offset):\n",
    "    xs = []\n",
    "    for i, v in enumerate(ys):\n",
    "        if i == 0:\n",
    "            xs.append(basex)\n",
    "            vm1 = v\n",
    "        else:\n",
    "            if abs(v - vm1) <= 1:\n",
    "                if xs[i-1] < basex:\n",
    "                    xs.append(basex + offset)\n",
    "                elif xs[i-1] == basex:\n",
    "                    xs[i-1] = basex - offset\n",
    "                    xs.append(basex + offset)\n",
    "                else:\n",
    "                    # previous version has been offset positively\n",
    "                    xs.append(basex - offset)\n",
    "            else:\n",
    "                xs.append(basex)\n",
    "            vm1 = v\n",
    "    \n",
    "    return xs\n",
    "\n",
    "def mask_wp2_atlas_data(cube, shp):\n",
    "    # mask wp2 data using shape file\n",
    "    \n",
    "    # approach varies depending on if cube is downloaded from WP2 atlas\n",
    "    # or direct from Glen's folders\n",
    "    if cube.ndim == 4:\n",
    "        # first get lat / lon mask over 2 dimensions\n",
    "        xy_mask = np.logical_not(shp.cube_intersection_mask(cube[0,:,:,0]))\n",
    "        # broadcast to 3d\n",
    "        xyp_mask = np.broadcast_to(xy_mask[:,:,np.newaxis], cube.shape[1:])\n",
    "        # broadcast to 4d\n",
    "        cube_mask = np.broadcast_to(xyp_mask, cube.shape)\n",
    "    else:\n",
    "        # 3 dimensional (lat, lon, percentile)\n",
    "        # get 2d mask\n",
    "        xy_mask = np.logical_not(shp.cube_intersection_mask(cube[:,:,0]))\n",
    "        # broadcast to 3d\n",
    "        cube_mask = np.broadcast_to(xy_mask[:,:,np.newaxis], cube.shape)\n",
    "\n",
    "    # apply to cube\n",
    "    # combine with existing mask\n",
    "    cube_mask = np.logical_or(cube_mask, cube.data.mask)\n",
    "    cube.data.mask = cube_mask\n",
    "\n",
    "    return cube\n",
    "\n",
    "\n",
    "def load_wp2_atlas(method, var, area, season):\n",
    "    # load netCDF file\n",
    "    base_path = \"/net/home/h02/tcrocker/code/EUCP_WP5_Lines_of_Evidence/weighting_data/WP2_atlas\"\n",
    "\n",
    "    # define region constraint if lat and lon supplied\n",
    "    if type(area) == list:\n",
    "        region = iris.Constraint(\n",
    "            longitude = lambda x: area[0] <= x <= area[1],\n",
    "            latitude = lambda x: area[2] <= x <= area[3]\n",
    "        )\n",
    "\n",
    "    bxp_obs = []\n",
    "    for data in [\"cons\", \"uncons\"]:\n",
    "        fname = f\"{base_path}/atlas_EUCP_{method}_{data}_{var}.nc\"\n",
    "        \n",
    "        cube = iris.load_cube(fname)\n",
    "\n",
    "        # extract shape / region\n",
    "        if type(area) == list:\n",
    "            cube = cube.extract(region)\n",
    "        else:\n",
    "            cube = mask_wp2_atlas_data(cube, area)\n",
    "\n",
    "        if season == \"JJA\":\n",
    "            # use first time point (JJA)\n",
    "            cube = cube[0]\n",
    "        elif season == \"DJF\":\n",
    "            cube = cube[1]\n",
    "        else:\n",
    "            raise ValueError(\"Only JJA and DJF available.\")\n",
    "\n",
    "        # area average\n",
    "        cube.coord(\"latitude\").units = \"degrees\"\n",
    "        cube.coord(\"latitude\").guess_bounds()\n",
    "        cube.coord(\"longitude\").units = \"degrees\"\n",
    "        cube.coord(\"longitude\").guess_bounds()\n",
    "        grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "        cube_mean = cube.collapsed([\"latitude\", \"longitude\"], iris.analysis.MEAN, weights=grid_areas)\n",
    "\n",
    "        # create boxplot stats object\n",
    "        if data == \"cons\":\n",
    "            label = method\n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        bxp_stats = {\n",
    "            \"whislo\": cube_mean.extract(iris.Constraint(percentile=10)).data.item(),\n",
    "            \"q1\": cube_mean.extract(iris.Constraint(percentile=25)).data.item(),\n",
    "            \"med\": cube_mean.extract(iris.Constraint(percentile=50)).data.item(),\n",
    "            \"q3\": cube_mean.extract(iris.Constraint(percentile=75)).data.item(),\n",
    "            \"whishi\": cube_mean.extract(iris.Constraint(percentile=90)).data.item(),\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "        bxp_obs.append(bxp_stats)\n",
    "\n",
    "    return bxp_obs\n",
    "\n",
    "def load_wp2_glen(var, area, season):\n",
    "    # Load WP2 constraint data from files in Glen's user space.\n",
    "    # define constraint if using a rectangle\n",
    "    if type(area) == list:\n",
    "        region = iris.Constraint(\n",
    "            longitude = lambda x: area[0] <= x <= area[1],\n",
    "            latitude = lambda x: area[2] <= x <= area[3]\n",
    "        )\n",
    "    \n",
    "    results = []\n",
    "    season = season.lower()\n",
    "    for d_type in [\"all\", \"prior\"]:\n",
    "        file_name = f\"/data/users/hadgh/eucp/data/v13/d23map/{var}Anom/{season}/{var}Anom_rcp85_eu_300km_W{d_type}-N600000-P21_cdf_b9514_20y_{season}_20401201-20601130.nc\"\n",
    "\n",
    "        cube = iris.load_cube(file_name)\n",
    "\n",
    "        # extract shape / region\n",
    "        if type(area) == list:\n",
    "            cube = cube.extract(region)\n",
    "        else:\n",
    "            cube = mask_wp2_atlas_data(cube, area)\n",
    "\n",
    "        grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "        cube_mean = cube.collapsed([\"latitude\", \"longitude\"], iris.analysis.MEAN, weights=grid_areas)\n",
    "\n",
    "        # create boxplot stats object\n",
    "        if d_type == \"all\":\n",
    "            label = \"UKCP constraint\"\n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        bxp_stats = {\n",
    "            \"whislo\": cube_mean.extract(iris.Constraint(percentile=10)).data.item(),\n",
    "            \"q1\": cube_mean.extract(iris.Constraint(percentile=25)).data.item(),\n",
    "            \"med\": cube_mean.extract(iris.Constraint(percentile=50)).data.item(),\n",
    "            \"q3\": cube_mean.extract(iris.Constraint(percentile=75)).data.item(),\n",
    "            \"whishi\": cube_mean.extract(iris.Constraint(percentile=90)).data.item(),\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "\n",
    "        results.append(bxp_stats)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def remove_institute_from_driver(driver_str):\n",
    "    # function to remove superfluous institute information from \n",
    "    # driving model supplied in CORDEX descriptions\n",
    "    INSTITUTES = [\n",
    "        'IPSL',\n",
    "        'NCC',\n",
    "        'MPI-M',\n",
    "        'CNRM-CERFACS',\n",
    "        'ICHEC',\n",
    "        'MOHC',\n",
    "        'KNMI',\n",
    "        'HCLIMcom',\n",
    "        'SMHI'\n",
    "    ]\n",
    "    # remove the institute bit from the \"driver\" string\n",
    "    new_str = driver_str\n",
    "    # loop through the institutes and remove them if found\n",
    "    for i in INSTITUTES:\n",
    "        i = '^' + i + '-'\n",
    "        new_str = re.sub(i, '', new_str)\n",
    "\n",
    "    if new_str == driver_str:\n",
    "        raise ValueError(f\"No institute found to remove from {driver_str}\")\n",
    "\n",
    "    return new_str\n",
    "\n",
    "\n",
    "def load_esmval_gridded_data(recipe, type, area, season):\n",
    "    # load gridded anomaly data that has been produced by the ESMValTool recipe\n",
    "    # recipe: name of recipe run that contains data, e.g. recipe_GCM_and_RCM_pan_EU_20211214_170431\n",
    "    # type: type of data to load, e.g. cmip5, cmip6, cordex, cpm, UKCP18 land-gcm, UKCP18 land-rcm\n",
    "    # area: area to compute area averages for. As a shape file for now..\n",
    "    # season: season to load data for, djf, mam, jja or son\n",
    "    # return an array of the computed area means for each data file (model) found\n",
    "\n",
    "    # first get path to where all the files will be\n",
    "    season = season.upper()\n",
    "    input_path = f\"/net/home/h02/tcrocker/code/EUCP_WP5_Lines_of_Evidence/esmvaltool/esmvaltool_output/{recipe}/work/gridded_anoms/main/{season}/\"\n",
    "\n",
    "    # setup land mask shape\n",
    "    lsm = shape.load_shp(\n",
    "        '/home/h02/tcrocker/code/EUCP_WP5_Lines_of_Evidence/shape_files/ne_110m_land/ne_110m_land.shp'\n",
    "        ).unary_union()\n",
    "    # need to reduce size of lsm to avoid bug in ascend\n",
    "    # see https://github.com/MetOffice/ascend/issues/8\n",
    "    corners = [(-30, 20), (-30, 75), (50, 75), (50, 20)]\n",
    "    rectangle = shape.create(corners, {'shape': 'rectangle'}, 'Polygon')\n",
    "    lsm = lsm.intersection(rectangle)\n",
    "\n",
    "    # process each file for the required datatype\n",
    "    fnames = glob(f\"{input_path}/{type}_*.nc\")\n",
    "\n",
    "    values = {}\n",
    "\n",
    "    for fname in fnames:\n",
    "        # ignore diff or mean files\n",
    "        if any([s in fname for s in ['diff', 'mean']]):\n",
    "            continue\n",
    "        \n",
    "        # load data\n",
    "        cube = iris.load_cube(fname)\n",
    "\n",
    "        # check proportion of area covered by valid cube data\n",
    "        # ignore data if this is less than 1\n",
    "        # i.e. there is not data for the whole of the shape\n",
    "        area_cov = check_data_shape_intersection(cube, area)\n",
    "        if area_cov < 0.9:\n",
    "            print(f\"WARNING: Data in {os.path.basename(fname)} only covers {area_cov * 100}% of supplied area\")\n",
    "            continue\n",
    "        if area_cov < 1.0:\n",
    "            print(f\"WARNING: Data in {os.path.basename(fname)} only covers {area_cov * 100}% of supplied area\")\n",
    "\n",
    "        # now mask data\n",
    "        # TODO - maybe turn whether / how to do this into an argument..\n",
    "        # could also achieve maskng via preprocessor functions from esmvaltool \n",
    "        # if it is necesary to run this outside the met office where ascend is not available\n",
    "        mask = lsm.intersection(area)\n",
    "        mask.mask_cube_inplace(cube)\n",
    "        # need some sort of logic to test if the cube contains data for all of the supplied area,\n",
    "        # if not we should reject it...\n",
    "        \n",
    "\n",
    "        # and compute weighted area average\n",
    "        # this is using weighted area weights from Ascend\n",
    "        awts = mask.cube_2d_weights(cube, False)\n",
    "        nwts = iris.analysis.cartography.area_weights(cube)\n",
    "\n",
    "        wts = awts.data * nwts\n",
    "        area_mean = cube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=wts)\n",
    "\n",
    "        # get model name etc. from filename\n",
    "        # want everything between the type and anom i.e.\n",
    "        # type_<this bit>_anom_season.nc\n",
    "        mname = os.path.basename(fname).split(f\"{type}_\")[1]\n",
    "        mname = mname.split(\"_anom\")[0]\n",
    "\n",
    "        values[mname] = area_mean.data.item()\n",
    "        \n",
    "    return values\n",
    "\n",
    "\n",
    "def check_data_shape_intersection(cube, shp):\n",
    "    # return proportion of grid boxes in shp (when put on same grid as cube)\n",
    "    # that have valid corresponding data in cube\n",
    "    \n",
    "    # check cube has a coord system, if not add one\n",
    "    if cube.coord('longitude').coord_system == None:\n",
    "        cube.coord('longitude').coord_system = iris.coord_systems.GeogCS(6371229.0)\n",
    "        cube.coord('latitude').coord_system = iris.coord_systems.GeogCS(6371229.0)\n",
    "\n",
    "    # first check intersection of cube bounding box with shape\n",
    "    # i.e. if the shape doesn't lie entirely inside the cube\n",
    "    # bounds we can return false immediately\n",
    "    int = shape.cube_bbox_shape_intersection(cube, shp)\n",
    "    diff = shp.difference(int)\n",
    "    if diff.data.area > 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    # now check intersection of cube data mask with shape\n",
    "    shape_mask = shp.cube_2d_weights(cube).data\n",
    "    data_mask = np.invert(cube.data.mask)\n",
    "\n",
    "    data_shape_intersection = shape_mask * data_mask\n",
    "\n",
    "    valid_boxes = np.sum(data_shape_intersection) / np.sum(shape_mask)\n",
    "\n",
    "    return valid_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = \"recipe_GCM_and_RCM_pan_EU_20220113_120253\"\n",
    "VAR = \"pr\"\n",
    "season = \"JJA\"\n",
    "AREA = \"SEE-3\"\n",
    "input_path = f\"/net/home/h02/tcrocker/code/EUCP_WP5_Lines_of_Evidence/esmvaltool/esmvaltool_output/{recipe}/work/gridded_anoms/main/{season}/\"\n",
    "area = shape.load_shp('/net/home/h02/tcrocker/code/EUCP_WP5_Lines_of_Evidence/shape_files/EUCP_WP3_domains/EUCP_WP3_domains.shp', name=AREA)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_shp: Invalid geometry ignored; ascend.Shape object, data: <class 'shapely.geometry.polygon.Polygon'>, attributes: featurecla: Land, ..., is_valid: False, coord_system: GeogCS(6371229.0)\n",
      "load_shp: Invalid geometry ignored; ascend.Shape object, data: <class 'shapely.geometry.polygon.Polygon'>, attributes: featurecla: Land, ..., is_valid: False, coord_system: GeogCS(6371229.0)\n",
      "load_shp: Invalid geometry ignored; ascend.Shape object, data: <class 'shapely.geometry.polygon.Polygon'>, attributes: featurecla: Land, ..., is_valid: False, coord_system: GeogCS(6371229.0)\n",
      "WARNING: Data in CORDEX_HCLIMcom-HCLIM38-ALADIN ICHEC-EC-EARTH_anom_JJA.nc only covers 96.72131147540983% of supplied area\n",
      "WARNING: Data in CORDEX_KNMI-RACMO23E KNMI-EC-EARTH_anom_JJA.nc only covers 39.34426229508197% of supplied area\n",
      "load_shp: Invalid geometry ignored; ascend.Shape object, data: <class 'shapely.geometry.polygon.Polygon'>, attributes: featurecla: Land, ..., is_valid: False, coord_system: GeogCS(6371229.0)\n",
      "WARNING: Data in cordex-cpm_SMHI-HCLIM38-AROME CEE-3_anom_JJA.nc only covers 14.754098360655737% of supplied area\n",
      "WARNING: Data in cordex-cpm_CLMcom-CMCC-CCLM5-0-9 ALP-3_anom_JJA.nc only covers 6.557377049180328% of supplied area\n",
      "WARNING: Data in cordex-cpm_GERICS-REMO2015 ALP-3_anom_JJA.nc only covers 6.557377049180328% of supplied area\n",
      "WARNING: Data in cordex-cpm_CNRM-AROME41t1 NWE-3_anom_JJA.nc only covers 0.0% of supplied area\n",
      "WARNING: Data in cordex-cpm_GERICS-REMO2015 CEU-3_anom_JJA.nc only covers 0.0% of supplied area\n",
      "WARNING: Data in cordex-cpm_ICTP-RegCM4-7 CEE-3_anom_JJA.nc only covers 0.0% of supplied area\n",
      "WARNING: Data in cordex-cpm_ICTP-RegCM4-7-0 ALP-3_anom_JJA.nc only covers 19.672131147540984% of supplied area\n",
      "WARNING: Data in cordex-cpm_CNRM-AROME41t1 ALP-3_anom_JJA.nc only covers 6.557377049180328% of supplied area\n",
      "WARNING: Data in cordex-cpm_COSMO-pompa ALP-3_anom_JJA.nc only covers 19.672131147540984% of supplied area\n",
      "WARNING: Data in cordex-cpm_HCLIMcom-HCLIM38-AROME ALP-3_anom_JJA.nc only covers 9.836065573770492% of supplied area\n",
      "WARNING: Data in cordex-cpm_GERICS-REMO2015 NEU-3_anom_JJA.nc only covers 0.0% of supplied area\n",
      "WARNING: Data in cordex-cpm_CLMcom-CMCC-CCLM5-0-9 SWE-3_anom_JJA.nc only covers 0.0% of supplied area\n",
      "WARNING: Data in cordex-cpm_HadREM3-RA-UM10.1 REU-2_anom_JJA.nc only covers 63.934426229508205% of supplied area\n",
      "WARNING: Data in cordex-cpm_KNMI-HCLIM38h1-AROME ALP-3_anom_JJA.nc only covers 8.19672131147541% of supplied area\n",
      "load_shp: Invalid geometry ignored; ascend.Shape object, data: <class 'shapely.geometry.polygon.Polygon'>, attributes: featurecla: Land, ..., is_valid: False, coord_system: GeogCS(6371229.0)\n",
      "load_shp: Invalid geometry ignored; ascend.Shape object, data: <class 'shapely.geometry.polygon.Polygon'>, attributes: featurecla: Land, ..., is_valid: False, coord_system: GeogCS(6371229.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/project/ukmo/scitools/opt_scitools/environments/default/2021_03_18-1/lib/python3.6/site-packages/iris/analysis/cartography.py:394: UserWarning: Using DEFAULT_SPHERICAL_EARTH_RADIUS.\n",
      "  warnings.warn(\"Using DEFAULT_SPHERICAL_EARTH_RADIUS.\")\n",
      "/net/project/ukmo/scitools/opt_scitools/environments/default/2021_03_18-1/lib/python3.6/site-packages/iris/analysis/cartography.py:394: UserWarning: Using DEFAULT_SPHERICAL_EARTH_RADIUS.\n",
      "  warnings.warn(\"Using DEFAULT_SPHERICAL_EARTH_RADIUS.\")\n",
      "/net/project/ukmo/scitools/opt_scitools/environments/default/2021_03_18-1/lib/python3.6/site-packages/iris/analysis/cartography.py:394: UserWarning: Using DEFAULT_SPHERICAL_EARTH_RADIUS.\n",
      "  warnings.warn(\"Using DEFAULT_SPHERICAL_EARTH_RADIUS.\")\n",
      "/net/project/ukmo/scitools/opt_scitools/environments/default/2021_03_18-1/lib/python3.6/site-packages/iris/analysis/cartography.py:394: UserWarning: Using DEFAULT_SPHERICAL_EARTH_RADIUS.\n",
      "  warnings.warn(\"Using DEFAULT_SPHERICAL_EARTH_RADIUS.\")\n",
      "/net/project/ukmo/scitools/opt_scitools/environments/default/2021_03_18-1/lib/python3.6/site-packages/iris/analysis/cartography.py:394: UserWarning: Using DEFAULT_SPHERICAL_EARTH_RADIUS.\n",
      "  warnings.warn(\"Using DEFAULT_SPHERICAL_EARTH_RADIUS.\")\n",
      "/net/project/ukmo/scitools/opt_scitools/environments/default/2021_03_18-1/lib/python3.6/site-packages/iris/analysis/cartography.py:394: UserWarning: Using DEFAULT_SPHERICAL_EARTH_RADIUS.\n",
      "  warnings.warn(\"Using DEFAULT_SPHERICAL_EARTH_RADIUS.\")\n",
      "/net/project/ukmo/scitools/opt_scitools/environments/default/2021_03_18-1/lib/python3.6/site-packages/iris/analysis/cartography.py:394: UserWarning: Using DEFAULT_SPHERICAL_EARTH_RADIUS.\n",
      "  warnings.warn(\"Using DEFAULT_SPHERICAL_EARTH_RADIUS.\")\n",
      "/net/project/ukmo/scitools/opt_scitools/environments/default/2021_03_18-1/lib/python3.6/site-packages/iris/analysis/cartography.py:394: UserWarning: Using DEFAULT_SPHERICAL_EARTH_RADIUS.\n",
      "  warnings.warn(\"Using DEFAULT_SPHERICAL_EARTH_RADIUS.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read data\n",
    "cmip5 = pd.DataFrame.from_dict(load_esmval_gridded_data(recipe, \"CMIP5\", area, season), orient=\"index\")\n",
    "cmip6 = pd.DataFrame.from_dict(load_esmval_gridded_data(recipe, \"CMIP6\", area, season), orient=\"index\")\n",
    "cordex = pd.DataFrame.from_dict(load_esmval_gridded_data(recipe, \"CORDEX\", area, season), orient=\"index\")\n",
    "cpm = pd.DataFrame.from_dict(load_esmval_gridded_data(recipe, \"cordex-cpm\", area, season), orient=\"index\")\n",
    "ukcp_g = pd.DataFrame.from_dict(load_esmval_gridded_data(recipe, \"UKCP18 land-gcm\", area, season), orient=\"index\")\n",
    "ukcp_r = pd.DataFrame.from_dict(load_esmval_gridded_data(recipe, \"UKCP18 land-rcm\", area, season), orient=\"index\")\n",
    "\n",
    "# List of models for Romania case study\n",
    "niculita_model_list = [\n",
    "    'RCA4 MPI-M-MPI-ESM-LR',\n",
    "    'RCA4 MOHC-HadGEM2-ES',\n",
    "    'RCA4 ICHEC-EC-EARTH',\n",
    "    'RCA4 CNRM-CERFACS-CNRM-CM5',\n",
    "    'REMO2009 MPI-M-MPI-ESM-LR',\n",
    "    'RACMO22E MOHC-HadGEM2-ES',\n",
    "    'RACMO22E ICHEC-EC-EARTH', \n",
    "    'HIRHAM5 ICHEC-EC-EARTH',\n",
    "    ]\n",
    "\n",
    "# list of models with evolving aerosols. See table B2 from:\n",
    "# GutiÃ©rrez, C., Somot, S., Nabat, P., Mallet, M., Corre, L., Van Meijgaard, E., et al. (2020). Future evolution of surface solar radiation and photovoltaic potential in Europe: investigating the role of aerosols. Environmental Research Letters, 15(3). https://doi.org/10.1088/1748-9326/ab6666\n",
    "aerosol_model_list = []\n",
    "for c in cordex.index:\n",
    "    if any([s in c for s in ['RACMO22E', 'ALADIN', 'HadREM3']]):\n",
    "        aerosol_model_list.append(c)\n",
    "\n",
    "# case_study_model_list = aerosol_model_list\n",
    "# case_study_model_list = niculita_model_list\n",
    "case_study_model_list = []\n",
    "\n",
    "# This dictionary maps CPM string to a RCM GCM string\n",
    "CPM_DRIVERS = {\n",
    "    'CNRM-AROME41t1': 'ALADIN63 CNRM-CERFACS-CNRM-CM5',\n",
    "    'CLMcom-CMCC-CCLM5-0-9': 'CCLM4-8-17 ICHEC-EC-EARTH',\n",
    "    'HCLIMcom-HCLIM38-AROME': 'HCLIMcom-HCLIM38-ALADIN ICHEC-EC-EARTH',\n",
    "    'GERICS-REMO2015': 'REMO2015 MPI-M-MPI-ESM-LR',\n",
    "    'COSMO-pompa': 'CCLM4-8-17 MPI-M-MPI-ESM-LR',\n",
    "    'ICTP-RegCM4-7-0': 'ICTP-RegCM4-7-0 MOHC-HadGEM2-ES',\n",
    "    'ICTP-RegCM4-7': 'ICTP-RegCM4-7-0 MOHC-HadGEM2-ES',\n",
    "    'KNMI-HCLIM38h1-AROME': 'KNMI-RACMO23E KNMI-EC-EARTH',\n",
    "    'SMHI-HCLIM38-AROME': 'SMHI-HCLIM38-ALADIN ICHEC-EC-EARTH',\n",
    "    'HadREM3-RA-UM10.1': 'MOHC-HadGEM3-GC3.1-N512 MOHC-HadGEM2-ES'\n",
    "}\n",
    "\n",
    "# List of CPM drivers from CORDEX to know which to plot as triangles\n",
    "cpm_driver_list = []\n",
    "for n in cpm.index:\n",
    "    cpm_driver_list.append(CPM_DRIVERS[n.split()[0]])\n",
    "\n",
    "# map WP2 data to GCM group\n",
    "wp2_methods = {\n",
    "    \"ETHZ_CMIP6_ClimWIP\": \"CMIP6\",\n",
    "    \"ICTP_CMIP6_REA\": \"CMIP6\",\n",
    "    \"ICTP_CMIP5_REA\": \"CMIP5\",\n",
    "    \"UKMO_CMIP6_UKCP\": \"UKCP_GCM\"\n",
    "}\n",
    "\n",
    "# set colours\n",
    "colour_map = {\n",
    "    \"CMIP6\": \"tab:blue\",\n",
    "    \"CMIP5\": \"tab:orange\",\n",
    "    \"CORDEX\": \"tab:green\",\n",
    "    \"CPM\": \"tab:red\",\n",
    "    \"UKCP_GCM\": \"tab:purple\",\n",
    "}\n",
    "\n",
    "# create data frame of just CPM drivers\n",
    "cpm_driver_df = cordex[cordex.index.isin(cpm_driver_list)]\n",
    "# create subset of models from case study\n",
    "case_study_df = cordex[cordex.index.isin(case_study_model_list)]\n",
    "\n",
    "# CMIP5 CORDEX drivers can be inferred directly from CORDEX model names\n",
    "cordex_driver_list = list(\n",
    "    set(\n",
    "        [remove_institute_from_driver(n.split(' ')[1]) for n in cordex.index]\n",
    "    )\n",
    ")\n",
    "cordex_driver_df = cmip5[cmip5.index.isin(cordex_driver_list)]\n",
    "\n",
    "# chuck everything in a dataframe for plotting\n",
    "plot_df = pd.DataFrame(\n",
    "    {\n",
    "        \"CMIP6\": cmip6[0],\n",
    "        \"CMIP5\": cmip5[0],\n",
    "        \"CORDEX Drivers\": cordex_driver_df[0],\n",
    "        \"CORDEX\": cordex[0],\n",
    "        \"CPM Drivers\": cpm_driver_df[0],\n",
    "        \"CPM\": cpm[0],\n",
    "        \"Case study models\": case_study_df[0],\n",
    "        \"UKCP_GCM\": ukcp_g[0],\n",
    "        \"UKCP_RCM\": ukcp_r[0],\n",
    "        \"UKCP Drivers\": ukcp_g[ukcp_g.index.isin(ukcp_r.index)][0],\n",
    "    }\n",
    ")\n",
    "\n",
    "# add case study models if using\n",
    "if case_study_model_list:\n",
    "    plot_df[\"Case study models\"] = case_study_df[0]\n",
    "\n",
    "\n",
    "\n",
    "# my calculated ClimWIP data - not using for now\n",
    "# pr_cmip5_climwip = iris.load_cube(CLIMWIP_PATH)\n",
    "\n",
    "# Set area to extract for plots\n",
    "area = shape.load_shp(\n",
    "    '/home/h02/tcrocker/code/EUCP_WP5_Lines_of_Evidence/shape_files/EUCP_WP3_domains/EUCP_WP3_domains.shp',\n",
    "    name = AREA\n",
    ")[0]\n",
    "# ALP-3\n",
    "# if AREA == \"ALP-3\":\n",
    "#     area = [1, 17, 40, 50]\n",
    "# else:\n",
    "# # CEE-3\n",
    "#     area = [18, 31, 41.5, 51.5]\n",
    "\n",
    "# load WP2 atlas constraint data\n",
    "constraint_data = {}\n",
    "for m in wp2_methods.keys():\n",
    "    constraint_data[m] = load_wp2_atlas(m, VAR, area, season)\n",
    "\n",
    "# also load Glen's UKCP data\n",
    "constraint_data[\"UKMO_CMIP6_UKCP\"] = load_wp2_glen(VAR, area, season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "# create figure and axes\n",
    "if len(case_study_model_list) > 0:\n",
    "    f, axs = plt.subplots(1,4, sharey=True, figsize=[19.2 ,  9.77], gridspec_kw={'width_ratios': [3, 2, 4, 1]})\n",
    "else:\n",
    "    f, axs = plt.subplots(1,3, sharey=True, figsize=[19.2 ,  9.77], gridspec_kw={'width_ratios': [3, 2, 4]})\n",
    "# f.suptitle(\"Projected % change in summer (JJA) rainfall for Romania. 2041-2060 vs 1995-2014. RCP8.5/ssp585\")\n",
    "# size of dots in swarm plots\n",
    "swarm_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'GCMs')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First panel\n",
    "# plot GCM boxes\n",
    "axs[0].clear()\n",
    "\n",
    "# plot boxes with matplotlib\n",
    "box_plot([cmip6[0], cmip5[0], ukcp_g[0]], axs[0], \"black\", \"None\", [0, 1, 2], [0.5, 0.5, 0.5])\n",
    "\n",
    "# plot dots\n",
    "sns.swarmplot(\n",
    "    data=plot_df[[\"CMIP6\", \"CMIP5\", \"UKCP_GCM\"]], ax=axs[0],\n",
    "    size=swarm_size, palette=[\"tab:blue\", \"tab:orange\", \"tab:purple\"],\n",
    "    alpha=0.75\n",
    "    )\n",
    "\n",
    "# last bits of formatting\n",
    "axs[0].axhline(linestyle=\":\", color=\"k\", alpha=0.5)\n",
    "plt.setp(axs[0].get_xticklabels(), rotation=45, ha=\"right\")\n",
    "axs[0].set_title(\"GCMs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Uncertainty estimates\\nfrom GCMs and observations')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot constrained ranges. 2nd panel\n",
    "axs[1].clear()\n",
    "for i, k in enumerate(constraint_data.keys()):\n",
    "    colour = colour_map[wp2_methods[k]]\n",
    "    # constrained\n",
    "    bxp([constraint_data[k][0]], axs[1], colour, 0.75, [i], 0.375)\n",
    "    # unconstrained\n",
    "    bxp([constraint_data[k][1]], axs[1], colour, 0.25, [i], 0.5)\n",
    "\n",
    "# This is for my custom ClimWIP data, not using for now\n",
    "# box_plot([pr_cmip5_climwip[2].data], axs[1], \"grey\", \"lightgrey\", [1], [0.375])\n",
    "# axs[0].boxplot(\n",
    "#     x=[cmip6[1], cmip5[1], pr_cmip5_climwip[2].data],\n",
    "#     positions=[0, 1, 1],\n",
    "#     whis=[10,90],\n",
    "#     showfliers=False,\n",
    "#     widths=[0.5, 0.5, 0.375],\n",
    "#     patch_artist=True\n",
    "# )\n",
    "\n",
    "axs[1].axhline(linestyle=\":\", color=\"k\", alpha=0.5)\n",
    "plt.setp(axs[1].get_xticklabels(), rotation=45, ha=\"right\")\n",
    "axs[1].set_title(\"Uncertainty estimates\\nfrom GCMs and observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Downscaled Projections')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third panel downscaled information\n",
    "axs[2].clear()\n",
    "sns.swarmplot(\n",
    "    data=plot_df[[\"CMIP5\", \"CORDEX\", \"CPM\", \"UKCP_GCM\", \"UKCP_RCM\"]], \n",
    "    ax=axs[2],\n",
    "    size=swarm_size,\n",
    "    palette=[\"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:purple\"]\n",
    ")\n",
    "# CORDEX drivers\n",
    "y = plot_df[\"CORDEX Drivers\"].dropna()\n",
    "x = create_x_points(y, 0.5, 0.05)\n",
    "axs[2].scatter(x, y, color=\"tab:orange\", marker=\">\", s=50)\n",
    "\n",
    "# CPM drivers\n",
    "y = plot_df[\"CPM Drivers\"].dropna()\n",
    "x = create_x_points(y, 1.5, 0.05)\n",
    "axs[2].scatter(x, y, color=\"tab:green\", marker=\">\", s=50)\n",
    "\n",
    "# Divider line for UKCP\n",
    "axs[2].axvline(2.5, color=\"lightgrey\")\n",
    "\n",
    "# UKCP drivers\n",
    "y = plot_df[\"UKCP Drivers\"].dropna()\n",
    "x = create_x_points(y, 3.5, 0.05)\n",
    "axs[2].scatter(x, y, color=\"tab:purple\", marker=\">\", s=50)\n",
    "\n",
    "# Final formatting etc.\n",
    "axs[2].axhline(linestyle=\":\", color=\"k\", alpha=0.5)\n",
    "plt.setp(axs[2].get_xticklabels(), rotation=45, ha=\"right\")\n",
    "axs[2].set_title(\"Downscaled Projections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra panel if a case study\n",
    "if len(case_study_model_list) > 0:\n",
    "    axs[3].clear()\n",
    "    sns.swarmplot(\n",
    "        data = plot_df[\"Case study models\"],\n",
    "        ax=axs[3],\n",
    "        size=swarm_size,\n",
    "        palette=[\"tab:green\"]\n",
    "        )\n",
    "    axs[3].set_xticklabels([\"Case study models\"])\n",
    "    axs[3].axhline(linestyle=\":\", color=\"k\", alpha=0.5)\n",
    "    plt.setp(axs[3].get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    axs[3].set_title(\"From study\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final figure spacing etc.\n",
    "# axs[0].set_ylabel(\"%\")\n",
    "plt.suptitle(f\"{AREA} {season} {VAR}\")\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.18, wspace=0.06)\n",
    "\n",
    "# save plot\n",
    "save_path = \"/home/h02/tcrocker/code/EUCP_WP5_Lines_of_Evidence/esmvaltool/plots\"\n",
    "plt.savefig(f\"{save_path}/{AREA}_{season}_{VAR}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ICTP-RegCM4-7 SEE-3</th>\n",
       "      <td>57.371185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0\n",
       "ICTP-RegCM4-7 SEE-3  57.371185"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6633e192c1a834d2fd65459fe5f8edb9aa82bc1737a4419b9833d7eca99ed278"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('2021_03_18-1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
